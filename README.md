# StatData Processor - Утилита для обработки бинарных данных

## Описание проекта

Утилита для анализа и обработки бинарных данных, представленных в виде структур `StatData`. Проект включает:

- Объединение двух массивов структур с уникальными ID
- Сортировку результатов по полю `cost`
- Сохранение и загрузку бинарных данных
- Форматированный вывод результатов

## Файлы проекта (gcc)

```
.
├── CMakeLists.txt          - Файл сборки CMake
├── src/
│   ├── main.c              - Основная программа
│   ├── statdata.c          - Реализация алгоритмов обработки
│   └── statdata.h          - Заголовочный файл с определениями
└── tests/
    └── all_tests.c         - Тесты для проверки функциональности

build.sh
```

## Сборка и запуск

### Сборка проекта

```bash
mkdir build && cd build
cmake ..
make
```

После сборки в папке `build` появятся два исполняемых файла:
- `statdata_processor` - основная утилита
- `statdata_tests` - тестовая утилита

### Запуск основной утилиты

```bash
./statdata_processor <файл1> <файл2> <выходной_файл>
```

### Запуск тестов

```bash
./statdata_tests
```

## Алгоритмы обработки данных

### Основной алгоритм (текущая реализация)

**Описание**:
1. Объединение двух массивов в один
2. Сортировка по полю `id` для группировки одинаковых ID
3. Линейное слияние записей с одинаковыми ID:
   - Суммирование полей `count` и `cost`
   - Логическое И для поля `primary`
   - Выбор максимального значения для `mode`
4. Сортировка результата по полю `cost`

**Асимптотическая сложность**:
- Объединение массивов: O(n)
- Сортировка по ID: O(n log n)
- Линейное слияние: O(n)
- Сортировка по cost: O(n log n)
- **Итого**: O(n log n)

**Преимущества**:
- Хорошая локальность данных
- Эффективное использование кеша процессора
- Оптимально для данных размером до 100-500k элементов

### Вариант с хеш-таблицей

**Описание**:
1. Создание хеш-таблицы (ключ - ID)
2. Построчное добавление элементов с обработкой коллизий
3. Извлечение значений из хеш-таблицы в массив
4. Сортировка по полю `cost`

**Асимптотическая сложность**:
- Вставка в хеш-таблицу: O(n) в среднем случае
- Извлечение данных: O(n)
- Сортировка по cost: O(n log n)
- **Итого**: O(n log n) в среднем случае

**Сравнение реализаций**:

| Критерий          | Текущая реализация | Хеш-таблица       |
|-------------------|--------------------|-------------------|
| Сложность         | O(n log n)         | O(n log n) средн. |
| Память            | O(n)               | O(n)              |
| Локальность данных| Отличная           | Плохая            |
| Лучший случай     | До 500k элементов  | >1M элементов     |
| Зависимости       | Нет                | Требуется uthash  |

## Выбор реализации

Для данного проекта была выбрана реализация с двойной сортировкой, так как:
1. Размер данных не превышает 100k элементов
2. Отсутствуют внешние зависимости
3. Гарантированная производительность без "проседаний" в худших случаях
4. Лучшая эффективность на небольших объемах данных

Для обработки значительно больших объемов данных (миллионы записей) рекомендуется реализация с хеш-таблицей.
 Практических сравнений времени не проводилось, при необходимости этого нужно будет отключить вывод логов в основной утилите. Несмотря на анализ ассимптотической сложности вариант с хэш имеет шанс оказаться быстрее... Лучшая проверка - практические замеры на реальном окружении конкретных данных.